{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcbf924",
   "metadata": {},
   "source": [
    "Simulated Patient "
   ]
  },
  {
   "cell_type": "code",
   "id": "97766649",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from textwrap import wrap\n",
    "from matplotlib import pyplot as plt\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e5738d7",
   "metadata": {},
   "source": [
    "from environment.fogg_behavioral_model import Patient"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "269a8cf9",
   "metadata": {},
   "source": [
    "updated_every_day = 24 \n",
    "week = 7 * updated_every_day"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cd7184e",
   "metadata": {},
   "source": [
    "eight_weeks = week * 8\n",
    "five_weeks = week * 5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62c8c037",
   "metadata": {},
   "source": [
    "# Basic stable condition\n",
    "condition = 'stable'\n",
    "habituation = False\n",
    "time_preference_update_step = 9999999999999999 #very high e.g. 9999999999 to make it out of the intervention time...effectively no preference shift\n",
    "\n",
    "# Other options (not considered in the AIBI project):\n",
    "# condition = 'habituated'\n",
    "# habituation=True\n",
    "# time_preference_update_step= 9999999999999999 \n",
    "\n",
    "# condition = 'changed_preference'\n",
    "# habituation=False\n",
    "# time_preference_update_step= five_weeks\n",
    "\n",
    "# condition = 'habituated_changed_preference'\n",
    "# habituation=True\n",
    "# time_preference_update_step= five_weeks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3fc885e2",
   "metadata": {},
   "source": [
    "# 500 runs in the paper\n",
    "runs = 50 "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5c0d39c",
   "metadata": {},
   "source": [
    "results_directory = 'results'\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d92ec5ad",
   "metadata": {},
   "source": [
    "# No interverntion"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d7630ab",
   "metadata": {},
   "source": [
    "sleep_list = []\n",
    "positive_list = []\n",
    "\n",
    "for run in range(0, runs):\n",
    "    \n",
    "    env1 = Patient(behavior_threshold=20, habituation=habituation, time_preference_update_step=time_preference_update_step)\n",
    "    action = 0\n",
    "    for i in  range(eight_weeks):\n",
    "        \n",
    "        observation, reward, done, _, info = env1.step(action)\n",
    "        action = 0\n",
    "    \n",
    "    \n",
    "    sleep_list.append(env1.h_slept)\n",
    "    positive_list.append(env1.h_positive)\n",
    "\n",
    "fig = plt.figure( figsize=(8, 5))\n",
    "plt.plot(np.nanmean(sleep_list, axis=0), label='Hours slept', color ='r')\n",
    "plt.plot(np.mean(positive_list, axis=0), label='Hours in positive mood')\n",
    "plt.ylabel('Hours ')\n",
    "plt.xlabel('Intevention Days')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "67f20644",
   "metadata": {},
   "source": [
    "# Intervention"
   ]
  },
  {
   "cell_type": "code",
   "id": "f70d2c89",
   "metadata": {},
   "source": [
    "def intervention(method, runs, name, condition):\n",
    "    rr_random_list = []\n",
    "    not_random_list = []\n",
    "    sleep_list = []\n",
    "    positive_list = []\n",
    "    performed = 0\n",
    "    for run in range(0, runs):\n",
    "\n",
    "        env1 = Patient(behavior_threshold=20, habituation=habituation, time_preference_update_step=time_preference_update_step)\n",
    "        env1 = method(env1)\n",
    "        if sum(env1.num_performed):\n",
    "            performed += 1\n",
    "        df = pd.DataFrame()\n",
    "        df['response_ratio'] = env1.rr\n",
    "        df['activity performed'] = env1.num_performed\n",
    "        df['notifications'] = env1.num_notified\n",
    "        df['sleep'] = env1.h_slept\n",
    "        df['positive'] = env1.h_positive\n",
    "        df['non_stationary'] = env1.h_nonstationary\n",
    "        \n",
    "        df.to_csv(\"{0}/patient1_{1}_{2}_run{3}.csv\".format(results_directory, condition, name, run))\n",
    "        rr_random_list.append(df.response_ratio)\n",
    "        not_random_list.append(df.notifications)\n",
    "        sleep_list.append(df.sleep)\n",
    "        positive_list.append(df.positive)\n",
    "\n",
    "    \n",
    "    print(\"This figures include runs in which no prompt resulted in the activity being performed\")\n",
    "    print(\"{0} out of {1} resulted in activity being performed \".format(performed, runs))\n",
    "    fig = plt.figure( figsize=(8, 5))\n",
    "    plt.plot(np.nanmean(np.array(sleep_list), axis=0), label='Hours slept', color ='r')\n",
    "    plt.plot(np.mean(positive_list, axis=0), label='Hours in positive mood')\n",
    "    plt.ylabel('Hours ')\n",
    "    plt.xlabel('Intevention Days')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=False, figsize=(10, 8))\n",
    "    ax1.plot(np.nanmean(rr_random_list, axis=0), label='response ratio', color ='r')\n",
    "    ax2.plot(np.nanmean(not_random_list, axis=0), label='num of notifications')\n",
    "    ax2.set_xlabel('Intervention days')\n",
    "    ax2.set_ylabel('No. of notifications')\n",
    "    ax1.set_ylabel('Response ratio')\n",
    "    plt.show()\n",
    "    return sleep_list, positive_list, rr_random_list, not_random_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "664f1c45",
   "metadata": {},
   "source": [
    "## Heuristic\n",
    "\n",
    "Notify every hour except in night and when the patient is alseep"
   ]
  },
  {
   "cell_type": "code",
   "id": "d42eec5d",
   "metadata": {},
   "source": [
    "def always_notify(env1, intervention_legth=eight_weeks):\n",
    "    \n",
    "    action = 0\n",
    "    for i in  range(intervention_legth):\n",
    "        \n",
    "        observation, _, _, _, _ = env1.step(action)\n",
    "        if observation[9] == 3 or observation[3] ==1:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "            \n",
    "    return env1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af434da5",
   "metadata": {},
   "source": [
    "sleep_listh, positive_listh, rr_h_list, not_h_list = intervention(always_notify, runs, 'h', condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "995f5503",
   "metadata": {},
   "source": [
    "## Random \n",
    "Randomly sample during the day skip the nights"
   ]
  },
  {
   "cell_type": "code",
   "id": "43bd0dad",
   "metadata": {},
   "source": [
    "def random_notification(env1, intervention_legth=eight_weeks):\n",
    "    \n",
    "    action = 0\n",
    "    for i in  range(intervention_legth):\n",
    "        \n",
    "        observation, _, _, _, _ = env1.step(action)\n",
    "        if observation[9] ==3:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = env1.action_space.sample()\n",
    "            \n",
    "    return env1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42bf1f55",
   "metadata": {},
   "source": [
    "%time _ = intervention(random_notification, runs, 'random', condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afb59536",
   "metadata": {},
   "source": [
    "def get_rr_and_notification(name, runs, condition):\n",
    "    \n",
    "    rr, noti = [],[]\n",
    "    failed = 0\n",
    "    for run in range(runs):\n",
    "        df = pd.read_csv(\"{0}/patient1_{1}_{2}_run{3}.csv\".format(results_directory,condition, name, run))\n",
    "        a_performed = sum(df['activity performed'].values)\n",
    "        if a_performed > 0: # activity performed at least once in the course of the intervention\n",
    "            rr.append(df.response_ratio)\n",
    "            noti.append(df.notifications)\n",
    "        else:\n",
    "            failed = failed +1 \n",
    "    print(\" {0} out of {1} runs had no activity performed throughout the full intervention.\".format(failed, runs))\n",
    "    return rr, noti, failed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f77d0766",
   "metadata": {},
   "source": [
    "rr_random_list, not_random_list, fppo = get_rr_and_notification('random', runs, condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04102ebc",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "### Static model"
   ]
  },
  {
   "cell_type": "code",
   "id": "7306fbc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "def supervised_after_three_weeks(env1):\n",
    "    \n",
    "    three_weeks = 24 * 7 * 3\n",
    "    env1 = always_notify(env1, intervention_legth=three_weeks)\n",
    "    clf = RandomForestClassifier(class_weight='balanced')\n",
    "    clf.fit(env1.observation_list, env1.activity_performed)\n",
    "    remaining_time = eight_weeks - three_weeks\n",
    "    observation = env1._get_current_state()\n",
    "    for i in range(remaining_time):\n",
    "        # applying supervised model\n",
    "        action = clf.predict(np.array([observation]))[0]\n",
    "        observation, _, _, _, _ = env1.step(action)\n",
    "        \n",
    "    return env1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d526b569",
   "metadata": {
    "tags": []
   },
   "source": [
    "%time _ = intervention(supervised_after_three_weeks,runs , 'static_sup3', condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d321278b",
   "metadata": {},
   "source": [
    "### Adaptive model "
   ]
  },
  {
   "cell_type": "code",
   "id": "3c4fee1e",
   "metadata": {},
   "source": [
    "def supervised_adaptive_after_three_weeks(env1):\n",
    "    \n",
    "    three_weeks = 24*7*3\n",
    "    env1 = always_notify(env1, intervention_legth=three_weeks)\n",
    "    clf = RandomForestClassifier(class_weight='balanced')\n",
    "    clf.fit(env1.observation_list, env1.activity_performed)\n",
    "    remaining_time = eight_weeks - three_weeks\n",
    "    observation = env1._get_current_state()\n",
    "    samples = len(env1.observation_list)\n",
    "    for i in range(remaining_time):\n",
    "        # applying supervised model\n",
    "        action = clf.predict(np.array([observation]))[0]\n",
    "        observation, _, _, _, _ = env1.step(action)\n",
    "        if len(env1.observation_list)> samples: # retrain when new samples are provided\n",
    "            clf = RandomForestClassifier(class_weight='balanced')\n",
    "            clf.fit(env1.observation_list, env1.activity_performed)\n",
    "            samples = len(env1.observation_list)\n",
    "            \n",
    "        \n",
    "    return env1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9926e2a4",
   "metadata": {},
   "source": [
    "%time _ = intervention(supervised_adaptive_after_three_weeks, runs, 'adaptive_sup3', condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62cb9d21",
   "metadata": {},
   "source": [
    "### How many times supervised training faild?"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d8111ca",
   "metadata": {},
   "source": [
    "rr_rf3_list, not_rf3_list, frf3 = get_rr_and_notification('static_sup3', runs, condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0792e0a",
   "metadata": {},
   "source": [
    "rr_rf3a_list, not_rf3a_list, frf3a = get_rr_and_notification('adaptive_sup3', runs, condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc35d037",
   "metadata": {},
   "source": [
    "## Deep Q Network"
   ]
  },
  {
   "cell_type": "code",
   "id": "69502d03",
   "metadata": {},
   "source": [
    "def dqn(env1):\n",
    "    model = DQN(\"MlpPolicy\", env1, verbose=0, learning_starts=updated_every_day)\n",
    "    model.learn(total_timesteps=eight_weeks)\n",
    "    return env1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49cb8e0e",
   "metadata": {},
   "source": [
    "%time _= intervention(dqn, runs, 'dqn', condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2b206c7",
   "metadata": {},
   "source": [
    "rr_dqn_list, not_dqn_list, fdqn = get_rr_and_notification('dqn', runs, condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95b0ad5a",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "id": "107e2d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "def ppo(env1):\n",
    "    model = PPO(\"MlpPolicy\", env1, verbose=0, n_steps=updated_every_day, batch_size=updated_every_day)\n",
    "    model.learn(total_timesteps=eight_weeks)\n",
    "    return env1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5043004c",
   "metadata": {},
   "source": [
    "%time _= intervention(ppo, runs, 'ppo', condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1449eef1",
   "metadata": {},
   "source": [
    "rr_ppo_list, not_ppo_list, fppo = get_rr_and_notification('ppo', runs, condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f89e8ad4",
   "metadata": {},
   "source": [
    "## Advantage Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "id": "7abf8150",
   "metadata": {
    "tags": []
   },
   "source": [
    "def a2c(env1):\n",
    "    model = A2C(\"MlpPolicy\", env1, verbose=0, n_steps=updated_every_day)\n",
    "    model.learn(total_timesteps=eight_weeks)\n",
    "    return env1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e3c199b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "%time _ = intervention(a2c, runs, 'a2c', condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79689682",
   "metadata": {},
   "source": [
    "rr_a2c_list, not_a2c_list, fppo = get_rr_and_notification('a2c', runs, condition)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bec5b166",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison between prompt learning stategies"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b8c7886",
   "metadata": {},
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2, 1, sharex=True, sharey=False, figsize=(10,10))\n",
    "ax1.plot(np.nanmean(rr_random_list, axis=0), label='Random')\n",
    "ax1.plot(np.nanmean(rr_rf3_list, axis=0), label='RF static')\n",
    "ax1.plot(np.nanmean(rr_rf3a_list, axis=0), label='RF adaptive')\n",
    "ax1.plot(np.nanmean(rr_dqn_list, axis=0), label='DQN')\n",
    "ax1.plot(np.nanmean(rr_ppo_list, axis=0), label='PPO')\n",
    "ax1.plot(np.nanmean(rr_a2c_list, axis=0), label='A2C')\n",
    "\n",
    "\n",
    "ax2.plot(np.nanmean(not_random_list, axis=0), label='Random')\n",
    "ax2.plot(np.nanmean(not_rf3_list, axis=0), label='RF static')\n",
    "ax2.plot(np.nanmean(not_rf3a_list, axis=0), label='RF adaptive')\n",
    "ax2.plot(np.nanmean(not_dqn_list, axis=0), label='DQN')\n",
    "ax2.plot(np.nanmean(not_ppo_list, axis=0), label='PPO')\n",
    "ax2.plot(np.nanmean(not_a2c_list, axis=0), label='A2C')\n",
    "ax2.plot(np.ones(len(np.mean(not_random_list, axis=0))) +2, label ='Preferred number of notifications a day')\n",
    "\n",
    "ax2.set_xlabel('Intervention Days', fontsize=16)\n",
    "ax2.set_ylabel('\\n'.join(wrap('Numbers of notifications', 20)), fontsize=16)\n",
    "ax1.set_ylabel('\\n'.join(wrap('Activity performed to prompt ratio', 20)) )\n",
    "# ax3.set_ylabel( '\\n'.join(wrap('Hours slept', 20)) )\n",
    "# plt.ylim(0, 0.95)\n",
    "ax2.legend(fontsize=13)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "300f2fbf",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
